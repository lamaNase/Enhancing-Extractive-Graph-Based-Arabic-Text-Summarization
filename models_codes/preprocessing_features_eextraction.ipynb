{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11006302,"sourceType":"datasetVersion","datasetId":6851919},{"sourceId":11932061,"sourceType":"datasetVersion","datasetId":6704451}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"color:#FF8888;\">🚀 Enhancing Graph-Based Arabic Extractive Text Summarization</span> using <span style=\"color:#1E90FF;\">Semantic</span> and <span style=\"color:#32CD32;\">Statistical</span> Features","metadata":{}},{"cell_type":"markdown","source":"# 📥 Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install camel-tools\n!pip install PyArabic\n!pip install KeyBERT\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:39:21.289948Z","iopub.execute_input":"2025-05-25T20:39:21.290344Z","iopub.status.idle":"2025-05-25T20:41:55.475590Z","shell.execute_reply.started":"2025-05-25T20:39:21.290306Z","shell.execute_reply":"2025-05-25T20:41:55.474202Z"}},"outputs":[{"name":"stdout","text":"Collecting camel-tools\n  Downloading camel_tools-1.5.6-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.0.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.17.0)\nCollecting docopt (from camel-tools)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from camel-tools) (5.5.0)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.13.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.2.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.3.8)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.5.1+cu121)\nCollecting transformers<4.44.0,>=4.0 (from camel-tools)\n  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.8.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.32.3)\nRequirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.14.1)\nCollecting pyrsistent (from camel-tools)\n  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.9.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from camel-tools) (4.67.1)\nCollecting muddler (from camel-tools)\n  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\nCollecting camel-kenlm>=2025.4.8 (from camel-tools)\n  Downloading camel-kenlm-2025.4.8.zip (556 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.5/556.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2->camel-tools) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.28.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.4.5)\nCollecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2025.1.31)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->camel-tools) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->camel-tools) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2->camel-tools) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2->camel-tools) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2->camel-tools) (2024.2.0)\nDownloading camel_tools-1.5.6-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\nDownloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: camel-kenlm, docopt\n  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for camel-kenlm: filename=camel_kenlm-2025.4.8-cp310-cp310-linux_x86_64.whl size=3453447 sha256=8128327bc05e96056b2d94261aeebedd2846893e8ceff04dde3dcfde4b773986\n  Stored in directory: /root/.cache/pip/wheels/c3/4d/7a/c08a4faa5066a526828b670f0a8a74d81b0bf940366174befe\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7c533962d2099e2547732dd93e347a151356fefd06dacfed39501533df73601e\n  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\nSuccessfully built camel-kenlm docopt\nInstalling collected packages: docopt, camel-kenlm, pyrsistent, muddler, tokenizers, transformers, camel-tools\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed camel-kenlm-2025.4.8 camel-tools-1.5.6 docopt-0.6.2 muddler-0.1.3 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4\nRequirement already satisfied: PyArabic in /usr/local/lib/python3.10/dist-packages (0.6.15)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic) (1.17.0)\nCollecting KeyBERT\n  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from KeyBERT) (1.26.4)\nRequirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from KeyBERT) (13.9.4)\nRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from KeyBERT) (1.2.2)\nRequirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from KeyBERT) (3.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->KeyBERT) (2.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->KeyBERT) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->KeyBERT) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->KeyBERT) (4.12.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->KeyBERT) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->KeyBERT) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->KeyBERT) (3.5.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->KeyBERT) (4.43.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->KeyBERT) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->KeyBERT) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->KeyBERT) (0.28.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->KeyBERT) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (2.32.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->KeyBERT) (0.1.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->KeyBERT) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->KeyBERT) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->KeyBERT) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->KeyBERT) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->KeyBERT) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->KeyBERT) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->KeyBERT) (0.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->KeyBERT) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->KeyBERT) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->KeyBERT) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.5->KeyBERT) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.5->KeyBERT) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->KeyBERT) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->KeyBERT) (2025.1.31)\nDownloading keybert-0.9.0-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: KeyBERT\nSuccessfully installed KeyBERT-0.9.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.43.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 📚 Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport nltk\nfrom nltk.stem.isri import ISRIStemmer\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport math\nfrom collections import defaultdict\nimport pyarabic.araby as araby\nimport torch\nfrom keybert import KeyBERT\nfrom transformers import AutoTokenizer, AutoModel\nimport numpy as np\nfrom tqdm import tqdm\nimport logging\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:41:55.477280Z","iopub.execute_input":"2025-05-25T20:41:55.477671Z","iopub.status.idle":"2025-05-25T20:42:29.094749Z","shell.execute_reply.started":"2025-05-25T20:41:55.477634Z","shell.execute_reply":"2025-05-25T20:42:29.093602Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 📂 Dataset Path","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/escs-dataset\"\n# Ensure the NLTK stopwords are downloaded\ntry:\n    arabic_stopwords = set(stopwords.words(\"arabic\"))\nexcept:\n    nltk.download(\"stopwords\")\n    arabic_stopwords = set(stopwords.words(\"arabic\"))\n\n# Output folders\noriginal_sentences_folder = \"original_sentences\"\npreprocessed_dl_folder = \"preprocessed_dl\"\npreprocessed_classical_folder = \"preprocessed_classical\"\n\n# Ensure directories exist\nos.makedirs(original_sentences_folder, exist_ok=True)\nos.makedirs(preprocessed_dl_folder, exist_ok=True)\nos.makedirs(preprocessed_classical_folder, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:42:29.097163Z","iopub.execute_input":"2025-05-25T20:42:29.097582Z","iopub.status.idle":"2025-05-25T20:42:29.108487Z","shell.execute_reply.started":"2025-05-25T20:42:29.097542Z","shell.execute_reply":"2025-05-25T20:42:29.107527Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 📊 Load the Data","metadata":{}},{"cell_type":"code","source":"# Load all files\ntexts = []\nfile_names = sorted(os.listdir(data_path))  # Ensure correct file order\n\nfor file_name in file_names:\n    file_path = os.path.join(data_path, file_name)\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        texts.append((file_name, file.read()))  # Store filename and content together\n\nprint(f\"Loaded {len(texts)} documents.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:42:29.109837Z","iopub.execute_input":"2025-05-25T20:42:29.110198Z","iopub.status.idle":"2025-05-25T20:42:29.610989Z","shell.execute_reply.started":"2025-05-25T20:42:29.110163Z","shell.execute_reply":"2025-05-25T20:42:29.610085Z"}},"outputs":[{"name":"stdout","text":"Loaded 153 documents.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 🔧 Stemming and Normalization Initalization","metadata":{}},{"cell_type":"code","source":"def ISRI_Stemmer(text):\n    #making an object\n    stemmer = ISRIStemmer()\n    \n    #stemming each word\n    text = stemmer.stem(text)\n    text = stemmer.pre32(text)\n    text = stemmer.suf32(text)\n    \n    return text\n\n# Arabic normalization function\ndef normalize_arabic(text):\n    text = text.strip()\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    \n    #remove repetetions\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = text.replace('وو', 'و')\n    text = text.replace('يي', 'ى')\n    text = text.replace('ييي', 'ى')\n    text = text.replace('اا', 'ا')\n\n    ## remove extra whitespace\n    text = re.sub('\\s+', ' ', text)\n    \n    # Remove longation\n    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) \n    \n    #Strip vowels from a text, include Shadda.\n    text = araby.strip_tashkeel(text)\n    \n    #Strip diacritics from a text, include harakats and small lettres The striped marks are\n    text = araby.strip_diacritics(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:42:29.611998Z","iopub.execute_input":"2025-05-25T20:42:29.612346Z","iopub.status.idle":"2025-05-25T20:42:29.620359Z","shell.execute_reply.started":"2025-05-25T20:42:29.612320Z","shell.execute_reply":"2025-05-25T20:42:29.618886Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# ⚙️ Pre-Processing Step","metadata":{}},{"cell_type":"code","source":"# Main processing loop\nfor file_name, text in tqdm(texts, total=len(texts), desc=\"Processing documents\"):\n    paragraphs = text.split(\"\\n\")  # Paragraph segmentation\n    \n    # Filter out empty paragraphs\n    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n    \n    original_sentences = {}\n    preprocessed_dl_sentences = {}\n    preprocessed_classical_sentences = {}\n    \n    for p_idx, paragraph in enumerate(paragraphs):\n        sentences = re.split(r\"[.?!]\", paragraph)  # Sentence segmentation\n        sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n        \n        # Skip paragraphs with no valid sentences\n        if not sentences:\n            continue\n            \n        original_sentences[str(p_idx)] = {str(s_idx): s for s_idx, s in enumerate(sentences)}\n        \n        # Normalize sentences for DL processing\n        normalized_sentences = {str(s_idx): normalize_arabic(s) for s_idx, s in enumerate(sentences)}\n        preprocessed_dl_sentences[str(p_idx)] = normalized_sentences\n        \n        # Remove stop words and apply stemming for classical representation\n        classical_sentences = {\n            str(s_idx): \" \".join(\n                [ISRI_Stemmer(word) for word in s.split() if word not in arabic_stopwords]\n            )\n            for s_idx, s in normalized_sentences.items()\n        }\n        preprocessed_classical_sentences[str(p_idx)] = classical_sentences\n    \n    # Skip files with no valid content\n    if not original_sentences:\n        print(f\"⚠️ Skipping empty file: {file_name}\")\n        continue\n        \n    # Save outputs with the same name as the original file, but with .json extension\n    base_filename = file_name.replace(\".txt\", \".json\")  # Replace .txt with .json\n    \n    with open(os.path.join(original_sentences_folder, base_filename), \"w\", encoding=\"utf-8\") as f:\n        json.dump(original_sentences, f, ensure_ascii=False, indent=4)\n    with open(os.path.join(preprocessed_dl_folder, base_filename), \"w\", encoding=\"utf-8\") as f:\n        json.dump(preprocessed_dl_sentences, f, ensure_ascii=False, indent=4)\n    with open(os.path.join(preprocessed_classical_folder, base_filename), \"w\", encoding=\"utf-8\") as f:\n        json.dump(preprocessed_classical_sentences, f, ensure_ascii=False, indent=4)\n\nprint(\"Preprocessing completed successfully!\")\n\n# Print the content of file1.json after preprocessing\nfile_to_print = \"file54.json\"\n\n# Load and print DL-preprocessed version\nwith open(os.path.join(preprocessed_dl_folder, file_to_print), \"r\", encoding=\"utf-8\") as f:\n    dl_data = json.load(f)\n    print(\"\\n📘 Preprocessed for Deep Learning (AraBERT) - \" + file_to_print)\n    print(json.dumps(dl_data, ensure_ascii=False, indent=4))\n\n# Load and print Classical-preprocessed version\nwith open(os.path.join(preprocessed_classical_folder, file_to_print), \"r\", encoding=\"utf-8\") as f:\n    classical_data = json.load(f)\n    print(\"\\n📗 Preprocessed for Classical Representation (TF-IDF etc.) - \" + file_to_print)\n    print(json.dumps(classical_data, ensure_ascii=False, indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:42:29.621419Z","iopub.execute_input":"2025-05-25T20:42:29.621717Z","iopub.status.idle":"2025-05-25T20:42:30.624609Z","shell.execute_reply.started":"2025-05-25T20:42:29.621690Z","shell.execute_reply":"2025-05-25T20:42:30.623414Z"}},"outputs":[{"name":"stderr","text":"Processing documents: 100%|██████████| 153/153 [00:00<00:00, 156.87it/s]","output_type":"stream"},{"name":"stdout","text":"Preprocessing completed successfully!\n\n📘 Preprocessed for Deep Learning (AraBERT) - file54.json\n{\n    \"0\": {\n        \"0\": \"ارتفاع الدولار\"\n    },\n    \"1\": {\n        \"0\": \"ارتفع الدولار امس بعد ان اظهر تقرير انخفاضا غير متوقع في العجز التجاري الامريكي في شهر مارس\"\n    },\n    \"2\": {\n        \"0\": \"وتقلص العجز الي 54\",\n        \"1\": \"99 مليار دولار في مارس من الرقم المعدل في فبراير وهو 60\",\n        \"2\": \"57 مليار دولار وجاء اقل بكثير من توقعات الاقتصادىن بعجز حجمه 61\",\n        \"3\": \"5 مليار دولار\"\n    },\n    \"3\": {\n        \"0\": \"وعلي مدي السنوات الثلاث الماضيه تقريبا ظل العجز التجاري مصدر ضغط كبير علي الدولار\",\n        \"1\": \"واذا واصل الامريكيون شراء بضاءع اجنبيه بمعدل اسرع من قدره الشركات الامريكيه علي بيع بضاءعها وخدماتها في الخارج فستظل حركه الدولار الي الخارج قويه مما يفرض ضغوطا علي العمله\"\n    },\n    \"4\": {\n        \"0\": \"وهبط اليورو الي ادني مستويات الجلسه عند 1\",\n        \"1\": \"2821 دولار بانخفاض حاد عن 1\",\n        \"2\": \"2875 دولار قبل صدور البيانات بقليل وبانخفاض 0\",\n        \"3\": \"4% عن مستوياتها في اواخر معاملات نيويورك يوم اول من امس\"\n    },\n    \"5\": {\n        \"0\": \"وامام الين صعد الدولار الي نحو 105\",\n        \"1\": \"70 ينات ارتفاعا من 105\",\n        \"2\": \"35 ينات قبل صدور التقرير وبارتفاع 0\",\n        \"3\": \"1% عن اواخر معاملات يوم اول من امس\"\n    }\n}\n\n📗 Preprocessed for Classical Representation (TF-IDF etc.) - file54.json\n{\n    \"0\": {\n        \"0\": \"رفع دولار\"\n    },\n    \"1\": {\n        \"0\": \"رفع دولار امس ان ظهر قرر خفض توقع عجز جري امر شهر\"\n    },\n    \"2\": {\n        \"0\": \"قلص عجز الي 54\",\n        \"1\": \"99 لير رقم عدل 60\",\n        \"2\": \"57 لير وجء اقل كثر وقع اقتصادىن عجز حجم 61\",\n        \"3\": \"5 لير\"\n    },\n    \"3\": {\n        \"0\": \"وعل مدي سنو ثلث اضه قرب ظل عجز جري صدر ضغط كبر علي دولار\",\n        \"1\": \"وذا وصل امر شرء ضءع جنب عدل سرع قدر شرك امر علي بيع ضءع خدم خرج تظل حرك دولار الي خرج قوه فرض ضغط علي عمل\"\n    },\n    \"4\": {\n        \"0\": \"هبط ورو الي ادن ستي جلس 1\",\n        \"1\": \"2821 خفض حاد 1\",\n        \"2\": \"2875 صدر بين قلل وبانخفاض 0\",\n        \"3\": \"4% ويا وخر عمل وير يوم اول امس\"\n    },\n    \"5\": {\n        \"0\": \"امم الن صعد دولار الي 105\",\n        \"1\": \"70 ينت رفع 105\",\n        \"2\": \"35 ينت صدر قرر وبارتفاع 0\",\n        \"3\": \"1% وخر عمل يوم اول امس\"\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 🗝️ Keyphrase Extraction","metadata":{}},{"cell_type":"code","source":"# Suppress verbose output\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Suppress tokenizer warnings\nwarnings.filterwarnings('ignore')  # Suppress general warnings\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Suppress transformers logs\n\n# Path to preprocessed files\npreprocessed_folder = \"/kaggle/working/preprocessed_dl\"\noutput_folder = \"/kaggle/working/sentence_scores\"\n\n# Create output folder if it doesn't exist\nos.makedirs(output_folder, exist_ok=True)\n\n# Load AraBERT model silently\nwith tqdm(total=1, desc=\"Loading AraBERT model\", leave=False) as pbar:\n    model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv02\", \n                                     return_dict=True)\n    kw_extractor = KeyBERT(model)\n    pbar.update(1)\n\n# Get all JSON files in the folder\njson_files = [f for f in os.listdir(preprocessed_folder) if f.endswith('.json')]\n\n# Process each file\nfor json_file in tqdm(json_files, desc=\"Processing files\", leave=True):\n    file_path = os.path.join(preprocessed_folder, json_file)\n    \n    # Load preprocessed sentences\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        preprocessed_data = json.load(f)\n    \n    # Flatten sentences while keeping original IDs\n    # Skip the first paragraph (title)\n    flattened_sentences = {}\n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        for sent_index, sentence in sentences.items():\n            sentence_id = f\"P{para_index}-S{sent_index}\"\n            flattened_sentences[sentence_id] = sentence\n    \n    # Total number of sentences (N) in this document\n    N = len(flattened_sentences)\n    if N == 0:\n        continue\n    \n    # Calculate sentence scores directly from KeyBERT scores\n    sentence_scores = {}\n    for sent_id, sentence in flattened_sentences.items():\n        if not sentence or len(sentence.strip()) == 0:\n            sentence_scores[sent_id] = 0\n            continue\n            \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            keywords = kw_extractor.extract_keywords(\n                sentence, \n                keyphrase_ngram_range=(1, 3), \n                top_n=5, \n                use_mmr=True, \n                diversity=0.7\n            )\n        \n        # Apply threshold of 0.5 to remove weak keyphrases\n        filtered_keywords = [kw for kw in keywords if kw[1] >= 0.5]\n        \n        # Sum the scores of filtered keyphrases\n        score = sum(kw[1] for kw in filtered_keywords)\n        \n        sentence_scores[sent_id] = score\n    \n    # Normalize scores for this document\n    if sentence_scores:\n        max_score = max(sentence_scores.values())\n        min_score = min(sentence_scores.values())\n        \n        # Avoid division by zero during normalization\n        score_range = max_score - min_score\n        normalized_scores = {}\n        \n        if score_range > 0:\n            for sent_id, score in sentence_scores.items():\n                normalized_scores[sent_id] = (score - min_score) / score_range\n        else:\n            # If all scores are the same, assign a default normalized value\n            for sent_id in sentence_scores:\n                normalized_scores[sent_id] = 0.5 if max_score > 0 else 0\n    else:\n        normalized_scores = {}\n    \n    # Prepare final result with only normalized scores, keeping paragraph/sentence structure\n    results = {}\n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        results[para_index] = {}\n        for sent_index, _ in sentences.items():\n            sent_id = f\"P{para_index}-S{sent_index}\"\n            results[para_index][sent_index] = normalized_scores.get(sent_id, 0)\n    \n    # Save results to output folder\n    output_file = os.path.join(output_folder, json_file)\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, ensure_ascii=False, indent=2)\n\nprint(\"✅ Processing completed. Normalized scores saved to:\", output_folder)\n# Print the content of file1.json after processing\nfile1_path = os.path.join(output_folder, \"file1.json\")\nif os.path.exists(file1_path):\n    with open(file1_path, \"r\", encoding=\"utf-8\") as f:\n        file1_content = json.load(f)\n    print(\"📄 Content of file1.json after processing:\")\n    print(json.dumps(file1_content, ensure_ascii=False, indent=2))\nelse:\n    print(\"⚠️ file1.json not found in output folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:42:30.626002Z","iopub.execute_input":"2025-05-25T20:42:30.626425Z","iopub.status.idle":"2025-05-25T20:55:43.478267Z","shell.execute_reply.started":"2025-05-25T20:42:30.626385Z","shell.execute_reply":"2025-05-25T20:55:43.476955Z"}},"outputs":[{"name":"stderr","text":"Loading AraBERT model:   0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abee1e42ff6419486d48f4cbc2041ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d366b72036945be9dd9549d6ba54931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460dcae4b9eb4964944fd828ddbb5695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8a5eef0cdf4d56a5bae0e7b2c7c691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5992d883325743c0addc46add2f8eaa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e2130603454e9ba635af1ae8cbc326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9578d21651a24bf3938b0e7362d9a71f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae3bd49254549899f523d21177cced3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ce0563834b47d3944dda0f252bca1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040ad3683a9144dbbf40c0d2500b9332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953e23943c8f477b81f87c89d1f863ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d9c0d79c5946ac9e411da766c7b3be"}},"metadata":{}},{"name":"stderr","text":"Processing files: 100%|██████████| 153/153 [12:39<00:00,  4.97s/it] ","output_type":"stream"},{"name":"stdout","text":"✅ Processing completed. Normalized scores saved to: /kaggle/working/sentence_scores\n📄 Content of file1.json after processing:\n{\n  \"1\": {\n    \"0\": 0.022355814390381354,\n    \"1\": 0.39531279353747883,\n    \"2\": 0.09952094683449182,\n    \"3\": 0.3387187676122488\n  },\n  \"2\": {\n    \"0\": 0.7082941950028179,\n    \"1\": 0.69007138831486\n  },\n  \"3\": {\n    \"0\": 0.03386248356190121,\n    \"1\": 1.0,\n    \"2\": 0.30922412173586317\n  },\n  \"4\": {\n    \"0\": 0.04086041705804996,\n    \"1\": 0.10046026676686079,\n    \"2\": 0.11370467781326318,\n    \"3\": 0.02977644185609619,\n    \"4\": 0.7734360323126058,\n    \"5\": 0.038793913206838254\n  },\n  \"5\": {\n    \"0\": 0.4143809881645689,\n    \"1\": 0.04936126244598909,\n    \"2\": 0.0,\n    \"3\": 0.29978395641555516,\n    \"4\": 0.3916964117978584,\n    \"5\": 0.36408040578621076,\n    \"6\": 0.15015029118917902\n  },\n  \"6\": {\n    \"0\": 0.0015498778884087778,\n    \"1\": 0.3529964305842569,\n    \"2\": 0.02174525643434154,\n    \"3\": 0.036774375352244995,\n    \"4\": 0.09247604734172457,\n    \"5\": 0.47430959984970883,\n    \"6\": 0.3323313920721398,\n    \"7\": 0.6369058801427767\n  }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 📏 Sentence Length Score","metadata":{}},{"cell_type":"code","source":"def calculate_entropy(text):\n    \"\"\"Calculate Shannon entropy of a text string\"\"\"\n    # Count the frequency of each character\n    chars = {}\n    for char in text:\n        if char in chars:\n            chars[char] += 1\n        else:\n            chars[char] = 1\n            \n    # Calculate entropy\n    length = len(text)\n    entropy = 0\n    for count in chars.values():\n        probability = count / length\n        entropy -= probability * math.log2(probability)\n    \n    return entropy\n\ndef calculate_sentence_length_scores(file_path):\n    \"\"\"Calculate sentence length scores for a specific file\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        preprocessed_data = json.load(f)\n    \n    # Find the longest sentence in the file (excluding title)\n    max_word_count = 0\n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        for _, sentence in sentences.items():\n            word_count = len(sentence.split())\n            max_word_count = max(max_word_count, word_count)\n    \n    # Calculate sentence length scores\n    paragraph_scores = {}\n    \n    # Store all scores for normalization\n    all_scores = []\n    \n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        sentence_scores = {}\n        for sent_index, sentence in sentences.items():\n            if not sentence or len(sentence.strip()) == 0:\n                sentence_scores[sent_index] = 0\n                continue\n                \n            word_count = len(sentence.split())\n            entropy = calculate_entropy(sentence)\n            \n            # Calculate length score = (word count / max word count) * entropy\n            length_score = (word_count / max_word_count) * entropy if max_word_count > 0 else 0\n            \n            sentence_scores[sent_index] = length_score\n            all_scores.append(length_score)\n            \n        paragraph_scores[para_index] = sentence_scores\n    \n    # Normalize scores within the file\n    if all_scores:\n        min_score = min(all_scores)\n        max_score = max(all_scores)\n        score_range = max_score - min_score\n        \n        # Normalize each score\n        if score_range > 0:\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    old_score = paragraph_scores[para_index][sent_index]\n                    normalized_score = (old_score - min_score) / score_range\n                    paragraph_scores[para_index][sent_index] = normalized_score\n        else:\n            # If all scores are the same, assign a default normalized value\n            default_value = 0.5 if max_score > 0 else 0\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    paragraph_scores[para_index][sent_index] = default_value\n    \n    return paragraph_scores\n\n# Process all files in the directory\npreprocessed_folder = \"/kaggle/working/preprocessed_classical\"\njson_files = [f for f in os.listdir(preprocessed_folder) if f.endswith('.json')]\n\n# Store all results in this dictionary\nall_length_scores = {}\n\n# Process each file\nfor json_file in tqdm(json_files, desc=\"Calculating sentence length scores\"):\n    file_path = os.path.join(preprocessed_folder, json_file)\n    \n    # Calculate length scores for this file\n    scores = calculate_sentence_length_scores(file_path)\n    \n    # Store results in dictionary\n    all_length_scores[json_file] = scores\n\nprint(f\"✅ Completed sentence length scoring for {len(json_files)} files\")\nprint(f\"Dictionary structure: {len(all_length_scores)} files with sentence length scores\")\n\n# Print sentence length scores for file1.json if it exists\nfile_name = \"file1.json\"\nif file_name in all_length_scores:\n   print(f\"\\n📂 Sentence length scores for {file_name}:\")\n   print(json.dumps(all_length_scores[file_name], indent=4, ensure_ascii=False))\nelse:\n   print(f\"\\n⚠️ {file_name} not found in all_length_scores.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.479457Z","iopub.execute_input":"2025-05-25T20:55:43.479845Z","iopub.status.idle":"2025-05-25T20:55:43.577268Z","shell.execute_reply.started":"2025-05-25T20:55:43.479807Z","shell.execute_reply":"2025-05-25T20:55:43.575920Z"}},"outputs":[{"name":"stderr","text":"Calculating sentence length scores: 100%|██████████| 153/153 [00:00<00:00, 1980.99it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Completed sentence length scoring for 153 files\nDictionary structure: 153 files with sentence length scores\n\n📂 Sentence length scores for file1.json:\n{\n    \"1\": {\n        \"0\": 0.15496433742876028,\n        \"1\": 0.1012784645871125,\n        \"2\": 0.003521049352458828,\n        \"3\": 0.0298484974453274\n    },\n    \"2\": {\n        \"0\": 0.20462843473223988,\n        \"1\": 0.017562921583262454\n    },\n    \"3\": {\n        \"0\": 0.24771776956673425,\n        \"1\": 0.0017208755876479136,\n        \"2\": 0.0\n    },\n    \"4\": {\n        \"0\": 0.33733690638358843,\n        \"1\": 0.21672022092956875,\n        \"2\": 0.22566138069294261,\n        \"3\": 1.0,\n        \"4\": 0.128471952189861,\n        \"5\": 0.1412206641517197\n    },\n    \"5\": {\n        \"0\": 0.32707616210297885,\n        \"1\": 0.4378944863193773,\n        \"2\": 0.0702149055094712,\n        \"3\": 0.29907952062884097,\n        \"4\": 0.11482904026653701,\n        \"5\": 0.11933947984560252,\n        \"6\": 0.07604993227117007\n    },\n    \"6\": {\n        \"0\": 0.502609108543862,\n        \"1\": 0.11802511542741742,\n        \"2\": 0.4747336090458656,\n        \"3\": 0.1162901044042386,\n        \"4\": 0.12094079584997566,\n        \"5\": 0.1416440979033531,\n        \"6\": 0.1760198308069259,\n        \"7\": 0.05054950361474116\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 📍 Sentence Location Score","metadata":{}},{"cell_type":"code","source":"def calculate_sentence_location_scores(file_path):\n    \"\"\"Calculate sentence location scores for a specific file\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        preprocessed_data = json.load(f)\n    \n    paragraph_scores = {}\n    all_scores = []  # To collect all scores for normalization\n    \n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        # Convert paragraph index to integer and add 1 to avoid division by zero\n        p_idx = int(para_index)\n        \n        sentence_scores = {}\n        for sent_index, _ in sentences.items():\n            # Convert sentence index to integer\n            s_idx = int(sent_index) + 1\n            \n            # Calculate location score based on position\n            if s_idx == 1:  # First sentence in paragraph\n                location_score = 1 / p_idx\n            else:  # Not the first sentence\n                location_score = 1 / (p_idx * s_idx)\n            \n            sentence_scores[sent_index] = location_score\n            all_scores.append(location_score)  # Add to collection for normalization\n            \n        paragraph_scores[para_index] = sentence_scores\n    \n    # Normalize scores within the file\n    if all_scores:\n        min_score = min(all_scores)\n        max_score = max(all_scores)\n        score_range = max_score - min_score\n        \n        # Normalize each score\n        if score_range > 0:\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    old_score = paragraph_scores[para_index][sent_index]\n                    normalized_score = (old_score - min_score) / score_range\n                    paragraph_scores[para_index][sent_index] = normalized_score\n        else:\n            # If all scores are the same, assign a default normalized value\n            default_value = 0.5 if max_score > 0 else 0\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    paragraph_scores[para_index][sent_index] = default_value\n    \n    return paragraph_scores\n\n# Process all files in the directory (using either preprocessed_dl or preprocessed_classical)\npreprocessed_folder = \"/kaggle/working/preprocessed_classical\"\njson_files = [f for f in os.listdir(preprocessed_folder) if f.endswith('.json')]\n\n# Store all results in this dictionary\nall_location_scores = {}\n\n# Process each file\nfor json_file in tqdm(json_files, desc=\"Calculating sentence location scores\"):\n    file_path = os.path.join(preprocessed_folder, json_file)\n    \n    # Calculate location scores for this file\n    scores = calculate_sentence_location_scores(file_path)\n    \n    # Store results in dictionary\n    all_location_scores[json_file] = scores\n\nprint(f\"✅ Completed sentence location scoring for {len(json_files)} files\")\nprint(f\"Dictionary structure: {len(all_location_scores)} files with sentence location scores\")\n\n# Print sentence length scores for file1.json if it exists\nfile_name = \"file1.json\"\nif file_name in all_location_scores:\n   print(f\"\\n📂 Sentence location scores for {file_name}:\")\n   print(json.dumps(all_location_scores[file_name], indent=4, ensure_ascii=False))\nelse:\n   print(f\"\\n⚠️ {file_name} not found in all_location_scores.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.579799Z","iopub.execute_input":"2025-05-25T20:55:43.580133Z","iopub.status.idle":"2025-05-25T20:55:43.611429Z","shell.execute_reply.started":"2025-05-25T20:55:43.580094Z","shell.execute_reply":"2025-05-25T20:55:43.610165Z"}},"outputs":[{"name":"stderr","text":"Calculating sentence location scores: 100%|██████████| 153/153 [00:00<00:00, 10863.13it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Completed sentence location scoring for 153 files\nDictionary structure: 153 files with sentence location scores\n\n📂 Sentence location scores for file1.json:\n{\n    \"1\": {\n        \"0\": 1.0,\n        \"1\": 0.4893617021276596,\n        \"2\": 0.3191489361702128,\n        \"3\": 0.23404255319148937\n    },\n    \"2\": {\n        \"0\": 0.4893617021276596,\n        \"1\": 0.23404255319148937\n    },\n    \"3\": {\n        \"0\": 0.3191489361702128,\n        \"1\": 0.14893617021276595,\n        \"2\": 0.0921985815602837\n    },\n    \"4\": {\n        \"0\": 0.23404255319148937,\n        \"1\": 0.10638297872340427,\n        \"2\": 0.06382978723404256,\n        \"3\": 0.04255319148936171,\n        \"4\": 0.029787234042553196,\n        \"5\": 0.02127659574468085\n    },\n    \"5\": {\n        \"0\": 0.18297872340425533,\n        \"1\": 0.08085106382978725,\n        \"2\": 0.04680851063829788,\n        \"3\": 0.029787234042553196,\n        \"4\": 0.019574468085106385,\n        \"5\": 0.012765957446808512,\n        \"6\": 0.007902735562310031\n    },\n    \"6\": {\n        \"0\": 0.14893617021276595,\n        \"1\": 0.06382978723404256,\n        \"2\": 0.03546099290780142,\n        \"3\": 0.02127659574468085,\n        \"4\": 0.012765957446808512,\n        \"5\": 0.0070921985815602835,\n        \"6\": 0.00303951367781155,\n        \"7\": 0.0\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 🔑 Cue Words Score","metadata":{}},{"cell_type":"code","source":"def load_cue_words(file_path):\n    \"\"\"Load cue words from file and normalize them\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        cue_words = [normalize_arabic(line.strip()) for line in f.readlines()]\n    return cue_words\n\ndef count_cue_words(text, cue_words):\n    \"\"\"Count cue words in normalized text\"\"\"\n    count = 0\n    for cue_word in cue_words:\n        # Count occurrences of the cue word in the normalized text\n        count += text.count(cue_word)\n    return count\n\ndef calculate_cue_word_scores(file_path, cue_words):\n    \"\"\"Calculate cue word scores for a specific file\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        preprocessed_data = json.load(f)\n    \n    paragraph_scores = {}\n    all_scores = []  # To collect all scores for normalization\n    \n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        # Count total cue words in the paragraph\n        paragraph_cue_count = 0\n        for _, sentence in sentences.items():\n            paragraph_cue_count += count_cue_words(sentence, cue_words)\n        \n        sentence_scores = {}\n        for sent_index, sentence in sentences.items():\n            # Count cue words in this sentence\n            sentence_cue_count = count_cue_words(sentence, cue_words)\n            \n            # Calculate cue word score\n            if paragraph_cue_count > 0:\n                cue_score = sentence_cue_count / paragraph_cue_count\n            else:\n                cue_score = 0\n            \n            sentence_scores[sent_index] = cue_score\n            all_scores.append(cue_score)  # Add to collection for normalization\n            \n        paragraph_scores[para_index] = sentence_scores\n    \n    # Normalize scores within the file\n    if all_scores:\n        min_score = min(all_scores)\n        max_score = max(all_scores)\n        score_range = max_score - min_score\n        \n        # Normalize each score\n        if score_range > 0:\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    old_score = paragraph_scores[para_index][sent_index]\n                    normalized_score = (old_score - min_score) / score_range\n                    paragraph_scores[para_index][sent_index] = normalized_score\n        else:\n            # If all scores are the same, assign a default normalized value\n            default_value = 0.5 if max_score > 0 else 0\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    paragraph_scores[para_index][sent_index] = default_value\n    \n    return paragraph_scores\n\n# Load cue words and normalize them\ncue_words_path = \"/kaggle/input/cue-words-list/arabic_cue_words.txt\"\ncue_words = load_cue_words(cue_words_path)\nprint(f\"Loaded {len(cue_words)} normalized cue words\")\n\n# For cue words, let's use the original_sentences folder since we want to match exact phrases\n# This will give us the most accurate count of cue words\npreprocessed_folder = \"/kaggle/working/preprocessed_dl\"\njson_files = [f for f in os.listdir(preprocessed_folder) if f.endswith('.json')]\n\n# Store all results in this dictionary\nall_cue_scores = {}\n\n# Process each file\nfor json_file in tqdm(json_files, desc=\"Calculating cue word scores\"):\n    file_path = os.path.join(preprocessed_folder, json_file)\n    \n    # Calculate cue word scores for this file\n    scores = calculate_cue_word_scores(file_path, cue_words)\n    \n    # Store results in dictionary\n    all_cue_scores[json_file] = scores\n\nprint(f\"✅ Completed cue word scoring for {len(json_files)} files\")\nprint(f\"Dictionary structure: {len(all_cue_scores)} files with cue word scores\")\n\n# Print sentence length scores for file1.json if it exists\nfile_name = \"file13.json\"\nif file_name in all_cue_scores:\n    print(f\"\\n📂 Sentence cue word scores for {file_name}:\")\n    print(json.dumps(all_cue_scores[file_name], indent=4, ensure_ascii=False))\nelse:\n    print(f\"\\n⚠️ {file_name} not found in all_cue_scores.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.612812Z","iopub.execute_input":"2025-05-25T20:55:43.613114Z","iopub.status.idle":"2025-05-25T20:55:43.800352Z","shell.execute_reply.started":"2025-05-25T20:55:43.613089Z","shell.execute_reply":"2025-05-25T20:55:43.798979Z"}},"outputs":[{"name":"stdout","text":"Loaded 80 normalized cue words\n","output_type":"stream"},{"name":"stderr","text":"Calculating cue word scores: 100%|██████████| 153/153 [00:00<00:00, 1000.17it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Completed cue word scoring for 153 files\nDictionary structure: 153 files with cue word scores\n\n📂 Sentence cue word scores for file13.json:\n{\n    \"1\": {\n        \"0\": 0.3333333333333333,\n        \"1\": 0.3333333333333333,\n        \"2\": 0.3333333333333333\n    },\n    \"2\": {\n        \"0\": 1.0,\n        \"1\": 0.0\n    },\n    \"3\": {\n        \"0\": 1.0,\n        \"1\": 0.0,\n        \"2\": 0.0,\n        \"3\": 0.0\n    },\n    \"4\": {\n        \"0\": 0.0,\n        \"1\": 0.0,\n        \"2\": 0.5,\n        \"3\": 0.5\n    },\n    \"5\": {\n        \"0\": 1.0\n    },\n    \"6\": {\n        \"0\": 0.0\n    },\n    \"7\": {\n        \"0\": 0.0\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# 🔢 Numarical Data Score","metadata":{}},{"cell_type":"code","source":"def count_numerical_data(text):\n    \"\"\"Count numerical data in text (both Arabic and English digits)\"\"\"\n    # Match both Arabic and English digits\n    pattern = r'[\\u0660-\\u0669\\d]+'\n    matches = re.findall(pattern, text)\n    return len(matches)\n\ndef calculate_numerical_data_scores(file_path):\n    \"\"\"Calculate numerical data scores for a specific file\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        preprocessed_data = json.load(f)\n    \n    paragraph_scores = {}\n    all_scores = []  # To collect all scores for normalization\n    \n    for para_index, sentences in preprocessed_data.items():\n        # Skip paragraph 0 (title)\n        if para_index == \"0\":\n            continue\n            \n        # Count total numerical data in the paragraph\n        paragraph_numerical_count = 0\n        for _, sentence in sentences.items():\n            paragraph_numerical_count += count_numerical_data(sentence)\n        \n        sentence_scores = {}\n        for sent_index, sentence in sentences.items():\n            # Count numerical data in this sentence\n            sentence_numerical_count = count_numerical_data(sentence)\n            \n            # Calculate numerical data score\n            if paragraph_numerical_count > 0:\n                numerical_score = sentence_numerical_count / paragraph_numerical_count\n            else:\n                numerical_score = 0\n            \n            sentence_scores[sent_index] = numerical_score\n            all_scores.append(numerical_score)  # Add to collection for normalization\n            \n        paragraph_scores[para_index] = sentence_scores\n    \n    # Normalize scores within the file\n    if all_scores:\n        min_score = min(all_scores)\n        max_score = max(all_scores)\n        score_range = max_score - min_score\n        \n        # Normalize each score\n        if score_range > 0:\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    old_score = paragraph_scores[para_index][sent_index]\n                    normalized_score = (old_score - min_score) / score_range\n                    paragraph_scores[para_index][sent_index] = normalized_score\n        else:\n            # If all scores are the same, assign a default normalized value\n            default_value = 0.5 if max_score > 0 else 0\n            for para_index in paragraph_scores:\n                for sent_index in paragraph_scores[para_index]:\n                    paragraph_scores[para_index][sent_index] = default_value\n    \n    return paragraph_scores\n\n# Process all files in the directory\npreprocessed_folder = \"/kaggle/working/original_sentences\"\njson_files = [f for f in os.listdir(preprocessed_folder) if f.endswith('.json')]\n\n# Store all results in this dictionary\nall_numerical_scores = {}\n\n# Process each file\nfor json_file in tqdm(json_files, desc=\"Calculating numerical data scores\"):\n    file_path = os.path.join(preprocessed_folder, json_file)\n    \n    # Calculate numerical data scores for this file\n    scores = calculate_numerical_data_scores(file_path)\n    \n    # Store results in dictionary\n    all_numerical_scores[json_file] = scores\n\nprint(f\"✅ Completed numerical data scoring for {len(json_files)} files\")\nprint(f\"Dictionary structure: {len(all_numerical_scores)} files with numerical data scores\")\n\n# Print sentence length scores for file1.json if it exists\nfile_name = \"file1.json\"\nif file_name in all_numerical_scores:\n    print(f\"\\n📂 Sentence numerical data scores for {file_name}:\")\n    print(json.dumps(all_numerical_scores[file_name], indent=4, ensure_ascii=False))\nelse:\n    print(f\"\\n⚠️ {file_name} not found in all_numerical_scores.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.801575Z","iopub.execute_input":"2025-05-25T20:55:43.801982Z","iopub.status.idle":"2025-05-25T20:55:43.865650Z","shell.execute_reply.started":"2025-05-25T20:55:43.801941Z","shell.execute_reply":"2025-05-25T20:55:43.864297Z"}},"outputs":[{"name":"stderr","text":"Calculating numerical data scores: 100%|██████████| 153/153 [00:00<00:00, 3360.49it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Completed numerical data scoring for 153 files\nDictionary structure: 153 files with numerical data scores\n\n📂 Sentence numerical data scores for file1.json:\n{\n    \"1\": {\n        \"0\": 0.5,\n        \"1\": 0.0,\n        \"2\": 0.0,\n        \"3\": 0.5\n    },\n    \"2\": {\n        \"0\": 0.0,\n        \"1\": 0.0\n    },\n    \"3\": {\n        \"0\": 0.0,\n        \"1\": 0.0,\n        \"2\": 1.0\n    },\n    \"4\": {\n        \"0\": 0.8,\n        \"1\": 0.2,\n        \"2\": 0.0,\n        \"3\": 0.0,\n        \"4\": 0.0,\n        \"5\": 0.0\n    },\n    \"5\": {\n        \"0\": 1.0,\n        \"1\": 0.0,\n        \"2\": 0.0,\n        \"3\": 0.0,\n        \"4\": 0.0,\n        \"5\": 0.0,\n        \"6\": 0.0\n    },\n    \"6\": {\n        \"0\": 1.0,\n        \"1\": 0.0,\n        \"2\": 0.0,\n        \"3\": 0.0,\n        \"4\": 0.0,\n        \"5\": 0.0,\n        \"6\": 0.0,\n        \"7\": 0.0\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# 📊 Final Statistical Score","metadata":{}},{"cell_type":"code","source":"# Combine all statistical scores (length, location, cue, numerical) and normalize\ndef combine_and_normalize_statistical_scores():\n    \"\"\"\n    Combine length, location, cue word, and numerical data scores.\n    Then normalize the combined scores within each document.\n    \"\"\"\n    # Dictionary to store all combined scores\n    all_combined_scores = {}\n    \n    # Get all files from one of the dictionaries (they should all have the same files)\n    json_files = all_length_scores.keys()\n    \n    # Process each file\n    for json_file in tqdm(json_files, desc=\"Combining statistical scores\"):\n        combined_scores = {}\n        \n        # Process paragraphs and sentences\n        for para_index in all_length_scores[json_file]:\n            combined_scores[para_index] = {}\n            \n            for sent_index in all_length_scores[json_file][para_index]:\n                # Sum all four statistical scores\n                length_score = all_length_scores[json_file][para_index].get(sent_index, 0)\n                location_score = all_location_scores[json_file][para_index].get(sent_index, 0)\n                cue_score = all_cue_scores[json_file][para_index].get(sent_index, 0)\n                numerical_score = all_numerical_scores[json_file][para_index].get(sent_index, 0)\n                \n                # Calculate total score (simple sum)\n                total_score = length_score + location_score + cue_score + numerical_score\n                \n                # Store the combined score\n                combined_scores[para_index][sent_index] = total_score\n        \n        # Normalize scores for this document\n        # Find min and max scores across all sentences in the document\n        # min_score = float('inf')\n        # max_score = float('-inf')\n        \n        # for para_scores in combined_scores.values():\n        #     for score in para_scores.values():\n        #         min_score = min(min_score, score)\n        #         max_score = max(max_score, score)\n        \n        # Normalize scores (handle case where all scores are the same)\n        # normalized_scores = {}\n        # score_range = max_score - min_score\n        \n        # for para_index, para_scores in combined_scores.items():\n        #     normalized_scores[para_index] = {}\n            \n        #     for sent_index, score in para_scores.items():\n        #         if score_range > 0:\n        #             normalized_score = (score - min_score) / score_range\n        #         else:\n        #             # If all scores are the same, assign a default value\n        #             normalized_score = 0.5 if max_score > 0 else 0\n                \n        #         normalized_scores[para_index][sent_index] = normalized_score\n        \n        # Store normalized scores in the final dictionary\n        all_combined_scores[json_file] = combined_scores\n    \n    return all_combined_scores\n\n# Calculate combined and normalized statistical scores\nall_statistical_scores = combine_and_normalize_statistical_scores()\n\nprint(f\"✅ Completed combining statistical scores for {len(all_statistical_scores)} files\")\nprint(f\"Dictionary structure: {len(all_statistical_scores)} files with combined statistical scores\")\n\n# Print sentence length scores for file1.json if it exists\nfile_name = \"file1.json\"\nif file_name in all_statistical_scores:\n    print(f\"\\n📂 Sentence  statistical scores for {file_name}:\")\n    print(json.dumps(all_statistical_scores[file_name], indent=4, ensure_ascii=False))\nelse:\n    print(f\"\\n⚠️ {file_name} not found in all_statistical_scores.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.866680Z","iopub.execute_input":"2025-05-25T20:55:43.867010Z","iopub.status.idle":"2025-05-25T20:55:43.885788Z","shell.execute_reply.started":"2025-05-25T20:55:43.866984Z","shell.execute_reply":"2025-05-25T20:55:43.883746Z"}},"outputs":[{"name":"stderr","text":"Combining statistical scores: 100%|██████████| 153/153 [00:00<00:00, 39512.87it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Completed combining statistical scores for 153 files\nDictionary structure: 153 files with combined statistical scores\n\n📂 Sentence  statistical scores for file1.json:\n{\n    \"1\": {\n        \"0\": 1.6549643374287604,\n        \"1\": 0.5906401667147722,\n        \"2\": 0.3226699855226716,\n        \"3\": 0.7638910506368168\n    },\n    \"2\": {\n        \"0\": 0.6939901368598995,\n        \"1\": 0.2516054747747518\n    },\n    \"3\": {\n        \"0\": 0.566866705736947,\n        \"1\": 0.15065704580041386,\n        \"2\": 1.0921985815602837\n    },\n    \"4\": {\n        \"0\": 1.371379459575078,\n        \"1\": 0.523103199652973,\n        \"2\": 0.28949116792698515,\n        \"3\": 1.0425531914893618,\n        \"4\": 0.1582591862324142,\n        \"5\": 0.16249725989640054\n    },\n    \"5\": {\n        \"0\": 2.3100548855072343,\n        \"1\": 1.3187455501491647,\n        \"2\": 0.11702341614776907,\n        \"3\": 0.32886675467139415,\n        \"4\": 0.13440350835164339,\n        \"5\": 0.532105437292411,\n        \"6\": 0.0839526678334801\n    },\n    \"6\": {\n        \"0\": 2.651545278756628,\n        \"1\": 1.18185490266146,\n        \"2\": 0.5101946019536671,\n        \"3\": 0.13756670014891945,\n        \"4\": 0.13370675329678416,\n        \"5\": 0.1487362964849134,\n        \"6\": 0.17905934448473745,\n        \"7\": 0.05054950361474116\n    }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 🗝️📊 Store both Keyphrase and statistical Scores","metadata":{}},{"cell_type":"code","source":"# Add statistical scores to the existing keyphrase score files\ndef combine_keyphrase_and_statistical_scores():\n    \"\"\"\n    Read keyphrase score files, add statistical scores, and save the updated files.\n    Each sentence will have two scores: keyphrase score and statistical score.\n    \"\"\"\n    # Path to the folder containing keyphrase scores\n    keyphrase_scores_folder = \"/kaggle/working/sentence_scores\"\n    \n    # Create output folder if it doesn't exist (we'll use the same folder)\n    os.makedirs(keyphrase_scores_folder, exist_ok=True)\n    \n    # Get all JSON files in the keyphrase scores folder\n    json_files = [f for f in os.listdir(keyphrase_scores_folder) if f.endswith('.json')]\n    \n    # Process each file\n    for json_file in tqdm(json_files, desc=\"Adding statistical scores to keyphrase files\"):\n        # Skip files that don't exist in the statistical scores dictionary\n        if json_file not in all_statistical_scores:\n            print(f\"⚠️ Skipping {json_file} - not found in statistical scores.\")\n            continue\n        \n        # Load the keyphrase scores file\n        file_path = os.path.join(keyphrase_scores_folder, json_file)\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            keyphrase_scores = json.load(f)\n        \n        # Create a new dictionary to store both scores\n        combined_scores = {}\n        \n        # Process each paragraph and sentence\n        for para_index, para_keyphrase_scores in keyphrase_scores.items():\n            combined_scores[para_index] = {}\n            \n            for sent_index, keyphrase_score in para_keyphrase_scores.items():\n                # Get the statistical score for this sentence\n                statistical_score = all_statistical_scores.get(json_file, {}).get(para_index, {}).get(sent_index, 0)\n                \n                # Store both scores in a dictionary\n                combined_scores[para_index][sent_index] = {\n                    \"keyphrase_score\": keyphrase_score,\n                    \"statistical_score\": statistical_score\n                }\n        \n        # Save the updated scores to the same file\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(combined_scores, f, ensure_ascii=False, indent=2)\n    \n    return f\"✅ Added statistical scores to {len(json_files)} keyphrase score files.\"\n\n# Execute the function\nresult_message = combine_keyphrase_and_statistical_scores()\nprint(result_message)\n\n# Verify structure of the updated files\nsample_file = [f for f in os.listdir(\"/kaggle/working/sentence_scores\") if f.endswith('.json')][0]\nwith open(os.path.join(\"/kaggle/working/sentence_scores\", sample_file), \"r\", encoding=\"utf-8\") as f:\n    sample_data = json.load(f)\n# Verify structure of file1.json after adding statistical scores\nfile1_path = os.path.join(\"/kaggle/working/sentence_scores\", \"file1.json\")\nif os.path.exists(file1_path):\n    with open(file1_path, \"r\", encoding=\"utf-8\") as f:\n        file1_content = json.load(f)\n    print(\"📄 Content of file1.json after adding statistical scores:\")\n    print(json.dumps(file1_content, ensure_ascii=False, indent=2))\nelse:\n    print(\"⚠️ file1.json not found in sentence_scores folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.887244Z","iopub.execute_input":"2025-05-25T20:55:43.887562Z","iopub.status.idle":"2025-05-25T20:55:43.966187Z","shell.execute_reply.started":"2025-05-25T20:55:43.887534Z","shell.execute_reply":"2025-05-25T20:55:43.964898Z"}},"outputs":[{"name":"stderr","text":"Adding statistical scores to keyphrase files: 100%|██████████| 153/153 [00:00<00:00, 2574.15it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Added statistical scores to 153 keyphrase score files.\n📄 Content of file1.json after adding statistical scores:\n{\n  \"1\": {\n    \"0\": {\n      \"keyphrase_score\": 0.022355814390381354,\n      \"statistical_score\": 1.6549643374287604\n    },\n    \"1\": {\n      \"keyphrase_score\": 0.39531279353747883,\n      \"statistical_score\": 0.5906401667147722\n    },\n    \"2\": {\n      \"keyphrase_score\": 0.09952094683449182,\n      \"statistical_score\": 0.3226699855226716\n    },\n    \"3\": {\n      \"keyphrase_score\": 0.3387187676122488,\n      \"statistical_score\": 0.7638910506368168\n    }\n  },\n  \"2\": {\n    \"0\": {\n      \"keyphrase_score\": 0.7082941950028179,\n      \"statistical_score\": 0.6939901368598995\n    },\n    \"1\": {\n      \"keyphrase_score\": 0.69007138831486,\n      \"statistical_score\": 0.2516054747747518\n    }\n  },\n  \"3\": {\n    \"0\": {\n      \"keyphrase_score\": 0.03386248356190121,\n      \"statistical_score\": 0.566866705736947\n    },\n    \"1\": {\n      \"keyphrase_score\": 1.0,\n      \"statistical_score\": 0.15065704580041386\n    },\n    \"2\": {\n      \"keyphrase_score\": 0.30922412173586317,\n      \"statistical_score\": 1.0921985815602837\n    }\n  },\n  \"4\": {\n    \"0\": {\n      \"keyphrase_score\": 0.04086041705804996,\n      \"statistical_score\": 1.371379459575078\n    },\n    \"1\": {\n      \"keyphrase_score\": 0.10046026676686079,\n      \"statistical_score\": 0.523103199652973\n    },\n    \"2\": {\n      \"keyphrase_score\": 0.11370467781326318,\n      \"statistical_score\": 0.28949116792698515\n    },\n    \"3\": {\n      \"keyphrase_score\": 0.02977644185609619,\n      \"statistical_score\": 1.0425531914893618\n    },\n    \"4\": {\n      \"keyphrase_score\": 0.7734360323126058,\n      \"statistical_score\": 0.1582591862324142\n    },\n    \"5\": {\n      \"keyphrase_score\": 0.038793913206838254,\n      \"statistical_score\": 0.16249725989640054\n    }\n  },\n  \"5\": {\n    \"0\": {\n      \"keyphrase_score\": 0.4143809881645689,\n      \"statistical_score\": 2.3100548855072343\n    },\n    \"1\": {\n      \"keyphrase_score\": 0.04936126244598909,\n      \"statistical_score\": 1.3187455501491647\n    },\n    \"2\": {\n      \"keyphrase_score\": 0.0,\n      \"statistical_score\": 0.11702341614776907\n    },\n    \"3\": {\n      \"keyphrase_score\": 0.29978395641555516,\n      \"statistical_score\": 0.32886675467139415\n    },\n    \"4\": {\n      \"keyphrase_score\": 0.3916964117978584,\n      \"statistical_score\": 0.13440350835164339\n    },\n    \"5\": {\n      \"keyphrase_score\": 0.36408040578621076,\n      \"statistical_score\": 0.532105437292411\n    },\n    \"6\": {\n      \"keyphrase_score\": 0.15015029118917902,\n      \"statistical_score\": 0.0839526678334801\n    }\n  },\n  \"6\": {\n    \"0\": {\n      \"keyphrase_score\": 0.0015498778884087778,\n      \"statistical_score\": 2.651545278756628\n    },\n    \"1\": {\n      \"keyphrase_score\": 0.3529964305842569,\n      \"statistical_score\": 1.18185490266146\n    },\n    \"2\": {\n      \"keyphrase_score\": 0.02174525643434154,\n      \"statistical_score\": 0.5101946019536671\n    },\n    \"3\": {\n      \"keyphrase_score\": 0.036774375352244995,\n      \"statistical_score\": 0.13756670014891945\n    },\n    \"4\": {\n      \"keyphrase_score\": 0.09247604734172457,\n      \"statistical_score\": 0.13370675329678416\n    },\n    \"5\": {\n      \"keyphrase_score\": 0.47430959984970883,\n      \"statistical_score\": 0.1487362964849134\n    },\n    \"6\": {\n      \"keyphrase_score\": 0.3323313920721398,\n      \"statistical_score\": 0.17905934448473745\n    },\n    \"7\": {\n      \"keyphrase_score\": 0.6369058801427767,\n      \"statistical_score\": 0.05054950361474116\n    }\n  }\n}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 📄📌 Here the folder \"/kaggle/working/sentence_scores\" must have both keyphrase scores and statistical scores for each sentence in each file","metadata":{}}]}